{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "FER.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8stOTbJNU83J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "90990626-7824-4126-98a7-0b12e27d9d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:30: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "<>:53: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "<>:30: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "<>:53: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-034f6ba49cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training_50580.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Training_50580.jpg'"
          ]
        }
      ],
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        
        
        "!unzip facedataset.zip\n",
        "\n",
        "import os\n",
        "training_list = os.listdir(\"FER_2013 dataset copy/train\")\n",
        "print(len(training_list))\n",
        "\n",
        "#training set\n",
        "\n",
        "class_labels = ['angry', 'happy', 'neutral', 'sad']\n",
        "\n",
        "fw = open('training_list.txt','w')\n",
        "\n",
        "for dirpath, dirname, filenames in os.walk('FER_2013 dataset/train'):\n",
        "  if 'FER_2013 dataset/train' is not dirpath:\n",
        "    print(dirpath)\n",
        "    dir_name = dirpath.split('/')[-1]\n",
        "    print(dir_name)\n",
        "\n",
        "    if dir_name in class_labels:\n",
        "      for f in filenames [:3000]:\n",
        "        print(dirpath, f)\n",
        "\n",
        "        fw.write(dirpath + '/' + f + '\\n')\n",
        "        \n",
        "fw.close()\n",
        "\n",
        "#testing set\n",
        "\n",
        "import os\n",
        "testing_list = os.listdir(\"FER_2013 dataset copy/test\")\n",
        "print(len(testing_list))\n",
        "print(testing_list)\n",
        "\n",
        "class_labels = ['angry', 'happy', 'neutral', 'sad']\n",
        "\n",
        "fw = open('testing_list.txt','w')\n",
        "\n",
        "for dirpath, dirname, filenames in os.walk('FER_2013 dataset/test'):\n",
        "  if 'FER_2013 dataset/test' is not dirpath:\n",
        "    print(dirpath)\n",
        "    dir_name = dirpath.split('/')[-1]\n",
        "    print(dir_name) \n",
        "\n",
        "    if dir_name in class_labels:\n",
        "      for f in filenames [:500]:\n",
        "        print(dirpath, f)\n",
        "\n",
        "        fw.write(dirpath + '/' + f + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "my_generator = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = my_generator.flow_from_directory('FER_2013 dataset copy/train/', target_size=(48,48), batch_size=16, class_mode='categorical')\n",
        "validate_generator = my_generator.flow_from_directory('FER_2013 dataset copy/test', target_size=(48,48), batch_size=16, class_mode='categorical')\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "for my_batch in train_generator:\n",
        "    images = my_batch[0]\n",
        "    labels = my_batch[1]\n",
        "    for i in range(len(labels)):\n",
        "      plt.imshow(images[i])\n",
        "      plt.colorbar()\n",
        "      plt.show()\n",
        "      print(images[i].shape)\n",
        "      print(labels[i])\n",
        "      break\n",
        "    break\n",
        "\n",
        "#deep learning convolutional neural network for image classification/machine learning\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "# the first argument represents number of input features. bigger number = more broad, smaller number = more detailed \n",
        "# padding arg makes the dimensions in the first convolved layer the same as original dimensions (48x48)\n",
        "model = Sequential()\n",
        "model.add( Conv2D( 64, ( 3, 3 ), activation = 'relu', input_shape = (48, 48, 3) ,  padding = 'same' ) )\n",
        "model.add( BatchNormalization() ) \n",
        "model.add( Conv2D( 64, ( 3, 3 ), activation = 'relu', padding = 'same' ) )\n",
        "model.add( BatchNormalization())\n",
        "model.add( MaxPool2D(2,2) )\n",
        "model.add( Conv2D( 32, ( 3, 3 ), activation = 'relu', padding = 'same' ) )\n",
        "model.add(Dropout(rate=0.4) )\n",
        "model.add( Conv2D( 32, ( 3, 3 ), activation = 'relu', padding = 'same' ) )\n",
        "model.add(BatchNormalization() )\n",
        "model.add( MaxPool2D(2,2) )\n",
        "model.add( Conv2D( 16, ( 3, 3 ), activation = 'relu', padding = 'same' ) )\n",
        "model.add(BatchNormalization() )\n",
        "model.add( Conv2D( 16, ( 3, 3 ), activation = 'relu', padding = 'same' ) )\n",
        "model.add(Dropout(rate = 0.4) )\n",
        "model.add( MaxPool2D(2,2) )\n",
        "model.add( Conv2D( 8, ( 3, 3 ), activation = 'relu', padding = 'same' ) )\n",
        "model.add(BatchNormalization() )\n",
        "model.add( Conv2D( 8, ( 3, 3 ), activation = 'relu', padding = 'same' ) )\n",
        "model.add(BatchNormalization() )\n",
        "model.add( Flatten() )\n",
        "# dense layer needs it to be converted into single dimensional\n",
        "model.add( Dense( 16, activation = 'relu' ) )\n",
        "model.add( Dense( 4, activation = 'softmax' ) )\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, BatchNormalization\n",
        "\n",
        "\n",
        "base_model = VGG16(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(48, 48, 3),\n",
        ")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add( Dense( 32, activation = 'relu' ) )\n",
        "model.add(BatchNormalization())\n",
        "model.add( Dense( 10, activation = 'relu' ) )\n",
        "model.add( Dense( 4, activation = 'softmax' ) )\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "model.compile( optimizer = 'adam', loss = 'categorical_crossentropy', metrics = [ 'accuracy' ] )\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "history = model.fit( train_generator, validation_data = validate_generator, epochs = 50, batch_size = 16, callbacks = [ModelCheckpoint(filepath = 'model_weights.hdf5', monitor = 'val_loss', save_best_only = True, save_weights_only = True, verbose = 1 )] )\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.ylim(0, 1.2)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.show()\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, BatchNormalization, Dropout\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu', input_shape = (48,48,3)))\n",
        "\n",
        "for i in range(16):\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters = 4, kernel_size = 3, activation = 'relu', padding='same'))\n",
        "    model.add(Dropout(rate=0.4))\n",
        "    model.add(Conv2D(filters = 4, kernel_size = 3, activation = 'relu', padding='same'))\n",
        "\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu'))\n",
        "model.add(MaxPool2D(pool_size = 2, strides = 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 4, activation = 'softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile( optimizer = 'adam', loss = 'categorical_crossentropy', metrics = [ 'accuracy' ] )\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "history = model.fit( train_generator, validation_data = validate_generator, epochs = 20, batch_size = 16, callbacks = [ModelCheckpoint(filepath = 'newmodel_weights.hdf5', monitor = 'val_loss', save_best_only = True, save_weights_only = True, verbose = 1 )] )\n",
        "\n",
        "def plot_learning_curves(history):\n",
        "    # print(history.params)\n",
        "    plt.clf()\n",
        "    if 'accuracy' in history.history:\n",
        "        plt.plot(history.history['accuracy'], 'g', label = 'Training Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], 'b', label = 'Validation Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "    \n",
        "    plt.ylim(0, 1.2)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_curves(history)"
      ]
    }
  ]
}
